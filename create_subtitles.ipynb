{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from moviepy.editor import *\n",
    "import speech_recognition as sr \n",
    "import cv2\n",
    "from googletrans import Translator\n",
    "from pydub import AudioSegment\n",
    "import simpleaudio as sa\n",
    "from scipy.io import wavfile\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of video file\n",
    "\n",
    "def get_duration_of_mp4(filename):\n",
    "    clip = VideoFileClip(filename)\n",
    "    duration = clip.duration\n",
    "    return duration\n",
    "\n",
    "# extract aufio from vide\n",
    "def extract_audio_from_mp4_to_wav(filename, path_to_safe_audio, start_time, end_time):\n",
    "    video = VideoFileClip(filename).subclip(start_time, end_time)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(path_to_safe_audio)\n",
    "    return audio\n",
    "\n",
    "def recognize_speech_from_file(audio_file):\n",
    "    # create a speech recognition object\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    # open the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "\n",
    "    # filter out the background noise\n",
    "    audio = audio.high_pass_filter(1000)\n",
    "\n",
    "    # boost the volume of the audio\n",
    "    audio = audio.apply_gain(20)\n",
    "\n",
    "    # convert the audio to a wave file\n",
    "    audio.export(\"filtered_audio.wav\", format=\"wav\")\n",
    "\n",
    "    # open the filtered audio file\n",
    "    with sr.AudioFile(\"filtered_audio.wav\") as source:\n",
    "        # read the audio file\n",
    "        audio_data = r.record(source)\n",
    "\n",
    "    # recognize the speech in the audio file\n",
    "    russian_text = r.recognize_google(audio_data, language='ru')\n",
    "    \n",
    "    # create a translation object\n",
    "    translator = Translator()\n",
    "\n",
    "    # translate the text from Russian to English\n",
    "    english_translation = translator.translate(russian_text, src='ru', dest='en').text\n",
    "    print(english_translation)\n",
    "\n",
    "def enhance_speech(audio_file):\n",
    "    # create a speech recognition object\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    # open the audio file\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "\n",
    "    # filter out the background noise\n",
    "    audio = audio.high_pass_filter(1000)\n",
    "\n",
    "    # boost the volume of the audio\n",
    "    audio = audio.apply_gain(20)\n",
    "\n",
    "    # save the enhanced audio to the \"audio\" subdirectory\n",
    "    audio.export(\"audio/enhanced_audio.wav\", format=\"wav\")\n",
    "\n",
    "def play_audio(audio_file):\n",
    "    # open the audio file\n",
    "    wave_obj = sa.WaveObject.from_wave_file(audio_file)\n",
    "\n",
    "    # play the audio\n",
    "    play_obj = wave_obj.play()\n",
    "    play_obj.wait_done()\n",
    "\n",
    "def remove_noise(input_data, output_data):\n",
    "    # load data\n",
    "    rate, data = wavfile.read(input_data)\n",
    "    # perform noise reduction\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "    wavfile.write(output_data, rate, reduced_noise)\n",
    "\n",
    "remove_noise('audio/audio0_till_60.wav', 'audio/enhanced_audio.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18136.06\n",
      "MoviePy - Writing audio in audio/audio0_till_60.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "0 : 0 - 60\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'array.array' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(i,\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m, start_time, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m, end_time)\n\u001b[1;32m     21\u001b[0m enhance_speech(path_to_safe_audio)\n\u001b[0;32m---> 22\u001b[0m remove_noise(\u001b[39m'\u001b[39;49m\u001b[39maudio/enhanced_audio.wav\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maudio/enhanced_audio.wav\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m play_audio(\u001b[39m'\u001b[39m\u001b[39maudio/enhanced_audio.wav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39m#subtitles_of_part = recognize_speech_from_file(path_to_safe_audio)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m#subtitles += subtitles_of_part\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#print(subtitles_of_part) \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 80\u001b[0m, in \u001b[0;36mremove_noise\u001b[0;34m(input_file, output_file, noise_threshold)\u001b[0m\n\u001b[1;32m     77\u001b[0m right_channel \u001b[39m=\u001b[39m audio\u001b[39m.\u001b[39msplit_to_mono()[\u001b[39m1\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[39m# Extract the frequency and time data for each channel\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m left_freq, left_time, left_spectrogram \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39;49mspectrogram(left_channel\u001b[39m.\u001b[39;49mget_array_of_samples(), \n\u001b[1;32m     81\u001b[0m                                                             audio\u001b[39m.\u001b[39;49mframe_rate)\n\u001b[1;32m     82\u001b[0m right_freq, right_time, right_spectrogram \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mspectrogram(right_channel\u001b[39m.\u001b[39mget_array_of_samples(), \n\u001b[1;32m     83\u001b[0m                                                                audio\u001b[39m.\u001b[39mframe_rate)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Set the noise floor to the specified threshold\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Development/Video Subtitles/.venv/lib/python3.8/site-packages/scipy/signal/_spectral_py.py:756\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munknown value for mode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, must be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    752\u001b[0m                      \u001b[39m.\u001b[39mformat(mode, modelist))\n\u001b[1;32m    754\u001b[0m \u001b[39m# need to set default for nperseg before setting default for noverlap below\u001b[39;00m\n\u001b[1;32m    755\u001b[0m window, nperseg \u001b[39m=\u001b[39m _triage_segments(window, nperseg,\n\u001b[0;32m--> 756\u001b[0m                                    input_length\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39;49mshape[axis])\n\u001b[1;32m    758\u001b[0m \u001b[39m# Less overlap than welch, so samples are more statisically independent\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[39mif\u001b[39;00m noverlap \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'array.array' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "duration = get_duration_of_mp4('Wedding.mp4')\n",
    "print(duration)\n",
    "parts = duration/(60)\n",
    "\n",
    "parts = int(parts)\n",
    "parts = list(range(parts))\n",
    "subtitles = ''\n",
    "for i in parts:\n",
    "    if i == 0:\n",
    "        start_time = 0\n",
    "        end_time = start_time + 60\n",
    "    elif i != parts[-1]:\n",
    "        start_time = end_time + 1\n",
    "        end_time = start_time + 60\n",
    "    else:\n",
    "        start_time = end_time + 1\n",
    "        end_time = duration\n",
    "    path_to_safe_audio = f'audio/audio_{start_time}_till_{end_time}.wav'\n",
    "    extract_audio_from_mp4_to_wav(filename='Wedding.mp4', path_to_safe_audio=path_to_safe_audio, start_time=start_time, end_time=end_time)\n",
    "    print(i,':', start_time, '-', end_time)\n",
    "    enhance_speech(path_to_safe_audio)\n",
    "    play_audio('audio/enhanced_audio.wav')\n",
    "    #subtitles_of_part = recognize_speech_from_file(path_to_safe_audio)\n",
    "    #subtitles += subtitles_of_part\n",
    "    \n",
    "    #print(subtitles_of_part) \n",
    "    \n",
    "print(subtitles)\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Apr  8 2021, 23:19:18) \n[Clang 12.0.5 (clang-1205.0.22.9)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2030f977e2d34d88a1587fc69be8557cd2a42e91ca7cd1f9ee9d6d26c113b1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
